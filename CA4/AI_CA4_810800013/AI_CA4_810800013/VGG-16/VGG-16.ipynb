{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAa6g3BcFCOi"
   },
   "source": [
    "# CNN: VGG-16 \n",
    "\n",
    "### CA4 @ AI Course\n",
    "\n",
    "*Full Name:* Shahin Shahnavaz\n",
    "\n",
    "*SID:* 810800013\n",
    "\n",
    "In this part of the assignment we want to do an image classification task using PyTorch on CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'AI/CA4'\n",
    "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "os.chdir(GOOGLE_DRIVE_PATH)\n",
    "print(os.listdir('./'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "A Convolutional Neural Network (CNN) architecture is a deep learning model designed for processing structured grid-like data, such as images. It consists of multiple layers, including convolutional, pooling, and fully connected layers. CNNs are highly effective for tasks like image classification, object detection, and image segmentation due to their hierarchical feature extraction capabilities.\n",
    "\n",
    "## VGG-16\n",
    "\n",
    "The VGG-16 model is a convolutional neural network (CNN) architecture that was proposed by the Visual Geometry Group (VGG) at the University of Oxford. It is characterized by its depth, consisting of 16 layers, including 13 convolutional layers and 3 fully connected layers. VGG-16 is renowned for its simplicity and effectiveness, as well as its ability to achieve strong performance on various computer vision tasks, including image classification and object recognition. The model’s architecture features a stack of convolutional layers followed by max-pooling layers, with progressively increasing depth. This design enables the model to learn intricate hierarchical representations of visual features, leading to robust and accurate predictions. Despite its simplicity compared to more recent architectures, VGG-16 remains a popular choice for many deep learning applications due to its versatility and excellent performance.\n",
    "\n",
    "The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is an annual competition in computer vision where teams tackle tasks including object localization and image classification. VGG16, proposed by Karen Simonyan and Andrew Zisserman in 2014, achieved top ranks in both tasks, detecting objects from 200 classes and classifying images into 1000 categories. This model achieves 92.7% top-5 test accuracy on the ImageNet dataset which contains 14 million images belonging to 1000 classes. \n",
    "\n",
    " \n",
    "![](https://i.postimg.cc/qR2ghvVg/VGG-16.png)\n",
    "\n",
    "The architecture of VGG-16 — Image from [Researchgate.net](https://www.researchgate.net/publication/321829624_Leaf_App_Leaf_recognition_with_deep_convolutional_neural_networks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a breakdown of the VGG-16 architecture:\n",
    "\n",
    "1. Convolutional Layers (64 filters, 3×3 filters, same padding):\n",
    "    - Two consecutive convolutional layers with 64 filters each and a filter size of 3×3.\n",
    "    - Same padding is applied to maintain spatial dimensions.\n",
    "2. Max Pooling Layer (2×2, stride 2):\n",
    "    - Max-pooling layer with a pool size of 2×2 and a stride of 2.\n",
    "3. Convolutional Layers (128 filters, 3×3 filters, same padding):\n",
    "    - Two consecutive convolutional layers with 128 filters each and a filter size of 3×3.\n",
    "4. Max Pooling Layer (2×2, stride 2):\n",
    "    - Max-pooling layer with a pool size of 2×2 and a stride of 2.\n",
    "5. Convolutional Layers (256 filters, 3×3 filters, same padding):\n",
    "    - Three consecutive convolutional layers with 256 filters each and a filter size of 3×3.\n",
    "6. Max Pooling Layer (2×2, stride 2):\n",
    "    - Max-pooling layer with a pool size of 2×2 and a stride of 2.\n",
    "7. Convolutional Layers (512 filters, 3×3 filters, same padding):\n",
    "    - Three sets of three consecutive convolutional layers with 512 filters each and a filter size of 3×3.\n",
    "8. Max Pooling Layer (2×2, stride 2):\n",
    "    - Max-pooling layer with a pool size of 2×2 and a stride of 2.\n",
    "9. Convolutional Layers (512 filters, 3×3 filters, same padding):\n",
    "    - Three sets of three consecutive convolutional layers with 512 filters each and a filter size of 3×3.\n",
    "10. Max Pooling Layer (2×2, stride 2):\n",
    "    - Max-pooling layer with a pool size of 2×2 and a stride of 2.\n",
    "11. Flattening:\n",
    "    - Flatten the output feature map.\n",
    "12. Fully Connected Layers:\n",
    "    - Three fully connected layers with ReLU activation.\n",
    "    - First layer with input size 512 and output size 4096.\n",
    "    - Second layer with input size 4096 and output size 4096.\n",
    "    - Third layer with input size 4096 and output size 10, corresponding to the 10 classes in the CIFAR10 dataset.\n",
    "    - Softmax activation is applied to the output of the third fully connected layer for classification.\n",
    "\n",
    "This architecture follows the specifications provided, including the use of ReLU activation function and the final fully connected layer outputting probabilities for 10 classes using softmax activation.\n",
    "\n",
    "source: [geeksforgeeks](https://www.geeksforgeeks.org/vgg-16-cnn-model/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Define your VGG16 model here from scratch (You are not allowed to use the existing models in pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       1,792\n",
      "|    └─ReLU: 2-2                         --\n",
      "|    └─Conv2d: 2-3                       36,928\n",
      "|    └─ReLU: 2-4                         --\n",
      "|    └─MaxPool2d: 2-5                    --\n",
      "|    └─Conv2d: 2-6                       73,856\n",
      "|    └─ReLU: 2-7                         --\n",
      "|    └─Conv2d: 2-8                       147,584\n",
      "|    └─ReLU: 2-9                         --\n",
      "|    └─MaxPool2d: 2-10                   --\n",
      "|    └─Conv2d: 2-11                      295,168\n",
      "|    └─ReLU: 2-12                        --\n",
      "|    └─Conv2d: 2-13                      590,080\n",
      "|    └─ReLU: 2-14                        --\n",
      "|    └─Conv2d: 2-15                      590,080\n",
      "|    └─ReLU: 2-16                        --\n",
      "|    └─MaxPool2d: 2-17                   --\n",
      "|    └─Conv2d: 2-18                      1,180,160\n",
      "|    └─ReLU: 2-19                        --\n",
      "|    └─Conv2d: 2-20                      2,359,808\n",
      "|    └─ReLU: 2-21                        --\n",
      "|    └─Conv2d: 2-22                      2,359,808\n",
      "|    └─ReLU: 2-23                        --\n",
      "|    └─MaxPool2d: 2-24                   --\n",
      "|    └─Conv2d: 2-25                      2,359,808\n",
      "|    └─ReLU: 2-26                        --\n",
      "|    └─Conv2d: 2-27                      2,359,808\n",
      "|    └─ReLU: 2-28                        --\n",
      "|    └─Conv2d: 2-29                      2,359,808\n",
      "|    └─ReLU: 2-30                        --\n",
      "|    └─MaxPool2d: 2-31                   --\n",
      "├─Sequential: 1-2                        --\n",
      "|    └─Linear: 2-32                      2,101,248\n",
      "|    └─ReLU: 2-33                        --\n",
      "|    └─Linear: 2-34                      16,781,312\n",
      "|    └─ReLU: 2-35                        --\n",
      "|    └─Linear: 2-36                      40,970\n",
      "=================================================================\n",
      "Total params: 33,638,218\n",
      "Trainable params: 33,638,218\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(VGG16(), input_size=(3, 32, 32), print=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train and test our model on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIv1Mpsbvt8v"
   },
   "source": [
    "## Device\n",
    "\n",
    "Set device to work with (GPU or CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "2uGuIUtwSFAR",
    "outputId": "a2289461-8220-4687-92c0-fb1fef4f6a5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4T4AL0cv1Jm"
   },
   "source": [
    "## Transforms & Dataset & Dataloader\n",
    "\n",
    "Here, you should download and load the dataset with the desire transforms. After that, you should split train dataset to train and validation sets. Finally, define the dataloaders for `train`, `validation` and `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4E6TT8whO9N4"
   },
   "outputs": [],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.491, 0.482, 0.446), std=(0.247, 0.243, 0.261)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.491, 0.482, 0.446), std=(0.247, 0.243, 0.261)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FR0BpY0YO-Em"
   },
   "outputs": [],
   "source": [
    "# inverse the normilize transform to restore the original data\n",
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = torch.tensor(mean)\n",
    "        self.std = torch.tensor(std)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be unnormalized.\n",
    "        Returns:\n",
    "            Tensor: Unnormalized image.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        return tensor\n",
    "\n",
    "norminv = UnNormalize(mean=(0.491, 0.482, 0.446), std=(0.247, 0.243, 0.261))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3UuPXFQOSDX",
    "outputId": "18d87b51-82b0-4dc9-e673-d77e7f3ed21f"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "initial_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "trainset, valset = random_split(initial_trainset, [45000, 5000])\n",
    "\n",
    "trainloader = None   # TODO\n",
    "valloader = None     # TODO\n",
    "testloader = None    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-C-YjLZtwnq2"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "Visualize 5 random images from each class in different columns\n",
    "\n",
    "- **Hint**:  You can use `plt.subplots` for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZyfRMlhqxii"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYp77Euaz_5u"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odR5mfCA0Eqy"
   },
   "source": [
    "### Model instantiation\n",
    "\n",
    "Create an instance of your model and move it to `device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3PjKY_oSBkg"
   },
   "outputs": [],
   "source": [
    "net = None     # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8zn5eLs0bBS"
   },
   "source": [
    "### Criterion & Optimizater\n",
    "\n",
    "Define `criterion` and `optimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEd5yXt3SL2T"
   },
   "outputs": [],
   "source": [
    "criterion = None    # TODO\n",
    "optimizer = None    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gth9e1k70uAI"
   },
   "source": [
    "### Train loop\n",
    "\n",
    "Train your model\n",
    "\n",
    "Tasks:\n",
    "- Things that are needed to be printed in each epoch:\n",
    "  - Number of epoch\n",
    "  - Train loss\n",
    "  - Train accuracy\n",
    "  - Validation loss\n",
    "  - Validation accuracy\n",
    "- Save train/validation loss and accuracy (of each epoch) in an array for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFz0lVkUs8Mk"
   },
   "outputs": [],
   "source": [
    "def train_epoch(net: torch.nn.Module, criterion: torch.nn.Module, optimizer: torch.optim.Optimizer ,dataloader: torch.utils.data.DataLoader):\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "def eval_epoch(net: torch.nn.Module, criterion: torch.nn.Module, dataloader: torch.utils.data.DataLoader):\n",
    "    # TODO\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "FhLUsf5bx3su",
    "outputId": "2081c8b9-79e2-4b2a-c6d3-a9b88ffddd6d"
   },
   "outputs": [],
   "source": [
    "epochs = None   # TODO\n",
    "history = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[]}\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwg7D6sv1kFL"
   },
   "source": [
    "### Visualize Loss and Accuracy plot\n",
    "\n",
    "Using the arrays that you have (from task 2 in the above section), visualize two plots: Accuracy plot (train and validation together) and Loss plot (train and validation together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 733
    },
    "id": "zoTWbAVUbJw_",
    "outputId": "125f4547-e7b5-4b30-bbfe-ca92c98edde0"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF_oCC3p2Q3C"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Test your trained model (using the Test Dataloader that you have). Our goal is to reach an accuracy above `70%`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzkNN4sMb3lK",
    "outputId": "2d185891-b69e-483c-8d0c-dec922928072"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
